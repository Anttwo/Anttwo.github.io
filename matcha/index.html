<!DOCTYPE html>
<html lang="en">
<head>
 <title>MAtCha</title>
 <meta name="description" content="MAtCha Gaussians: Atlas of Charts for High-Quality Geometry and Photorealism From Sparse Views"/>
 <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=yes">
 <meta charset="utf-8">
 <!-- <link rel="icon" href="./icon_unicorn.png" type="image/png"> -->

 <!--Facebook-->
 <meta property="og:image" content="">
 <meta property="og:image:type" content="image/jpg">
 <meta property="og:image:width" content="600">
 <meta property="og:image:height" content="400">
 <meta property="og:type" content="website"/>
 <meta property="og:url" content="https://anttwo.github.io/matcha/"/>
 <meta property="og:title" content="MAtCha"/>
 <meta property="og:description" content="Project page for MAtCha Gaussians: Atlas of Charts for High-Quality Geometry and Photorealism From Sparse Views"/>

 <!--Twitter-->
 <meta name="twitter:card" content="summary_large_image" />
 <meta name="twitter:title" content="MAtCha" />
 <meta property="twitter:description" content="MAtCha Gaussians: Atlas of Charts for High-Quality Geometry and Photorealism From Sparse Views."/>
 <meta name="twitter:image" content="">

 <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/css/bootstrap.min.css">
 <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
 <!-- <link href="https://fonts.googleapis.com/css?family=Montserrat:800|Roboto:400,700" rel="stylesheet"> -->
 <link rel="preconnect" href="https://fonts.googleapis.com">
 <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin> 
 <link href="https://fonts.googleapis.com/css2?family=Roboto:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&display=swap" rel="stylesheet"> 
 <!-- <link href="http://fonts.googleapis.com/css?family=Roboto:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&display=swap" rel="stylesheet">  -->
 
 <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.4.1/jquery.min.js"></script>
 <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/js/bootstrap.min.js"></script>

 <!-- <link href="style_2.css" rel="stylesheet"> -->
 <!-- <link href="style.css" rel="stylesheet"> -->
 <link href="../base2.css" rel="stylesheet">

<!--  Google Analytics (DO NOT copy/paste following section, setup your own analytics tag at https://analytics.google.com/analytics/web/)  -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-CDL4R10HKE"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-CDL4R10HKE');
</script>
<!-- End Google Analytics -->

<link href="https://fonts.googleapis.com/icon?family=Material+Icons" rel="stylesheet">
<link href="https://fonts.googleapis.com/icon?family=Material+Icons+Outlined" rel="stylesheet">

</head>
<body>

<div class="container" style="text-align:center; padding-top:2rem 15px">
 <div class="row" style="text-align:center">
   <h1 style="font-size: 60px;">MAtCha Gaussians: <br>Atlas of Charts for High-Quality Geometry and Photorealism From Sparse Views</h1>
   <h1 style="font-size: xxx-large;">arXiv 2024</h1>
 </div>
 <div class="row" style="text-align:center">
   <div class="col-xs-0 col-md-2"></div>
   <div class="col-xs-12 col-md-8">
     <h4 style="font-size: 22px;">
       <nobr><a href="https://anttwo.github.io/">Antoine Gu&eacutedon<sup>1</sup></a></nobr> &emsp;
       <nobr><a href="https://scholar.google.com/citations?user=-LzEJVwAAAAJ&hl=en">Tomoki Ichikawa<sup>2</sup></a></nobr> &emsp;
       <nobr><a href="https://kyamashita5.github.io/">Kohei Yamashita<sup>2</sup></a></nobr> &emsp;
       <nobr><a href="https://scholar.google.com/citations?user=SXXEZhYAAAAJ&hl=en">Ko Nishino<sup>2</sup></a></nobr>
     </h4>
     <p style="font-size: 20px; text-align: center;"><nobr><sup>1</sup>LIGM, Ecole des Ponts</nobr>, <nobr>Univ Gustave Eiffel</nobr>, CNRS</nobr>, France<br></p>
     <p style="font-size: 20px; text-align: center;"><nobr><sup>2</sup>Graduate School of Informatics</nobr>, Kyoto University</nobr>, Japan<br></p>
   </div>
   <!-- <div class="hidden-xs hidden-sm col-md-1" style="text-align:left; margin-left:0px; margin-right:0px">
     <a href="https://arxiv.org/abs/2303.03315" style="color:#448D87">
       <i class="fa fa-file-pdf-o fa-4x"></i></a>
   </div>
   <div class="hidden-xs hidden-sm col-md-1" style="text-align:left; margin-left:0px;">
     <a href="https://github.com/Anttwo/SuGaR" style="color:#448D87">
       <i class="fa fa-github fa-4x"></i></a>
   </div> -->
 </div>
</div>

<div class="container" style="text-align:center; padding:0rem">
 <div class="row">
   <div class="col-xs-12">
   <h3 style="text-align:center; padding-bottom:1rem">
     <a class="label label-info" href="https://arxiv.org/abs/2412.06767"><i class="fa fa-file-text"></i>&nbsp;&nbsp;Paper</a>
     <a class="label label-info" href="https://github.com/Anttwo/MAtCha"><i class="fa fa-code fa-lg" style="vertical-align: top; margin-top: 6px"></i>&nbsp;&nbsp;Code (Coming Soon)</a>
     <!-- <a class="label label-info" href="https://github.com/Anttwo/sugar_frosting_blender_addon/"><i class="fa fa-gamepad fa-lg" style="vertical-align:top; margin-top:5px"></i>&nbsp;&nbsp;Blender Add-On</a> -->
     <a class="label label-info" href="./ref.bib"><i class="fa fa-commenting fa-lg" style="vertical-align:top;margin-top:6px"></i>&nbsp;&nbsp;BibTeX</a>
   </h3>
   </div>
 </div>

 <div class="row" style="padding:2rem">
   <div class="col-xs-12">
   <!-- <video autoplay loop muted playsinline src="teaser.mp4" style="width:95%; max-width: 900px"></video> -->
   <!-- <iframe class="embed-responsive-item text-center" src="https://www.youtube.com/embed/MAkFyWfiBQo?si=mIN-4Wk3P2sWBG7C" frameborder="0"
          allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"
          style="width:100%; height:540px; clip-path:inset(1px 1px);" allowfullscreen></iframe> -->
  <!-- <iframe src="https://drive.google.com/file/d/1oov9S0Cw0LKr31M-bWpr1JAdG-fG49Er/preview"
     width=100% height="620px" frameborder="0" allow="autoplay" allowfullscreen></iframe> -->
  <iframe class="embed-responsive-item text-center" src="https://drive.google.com/file/d/1eisdCFnWs8cAI8flOJ93ZQ8tkqSZmcEB/preview" frameborder="0"
     allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"
     style="width:100%; height:620px; clip-path:inset(1px 1px);" allowfullscreen></iframe>
  <!-- <i><b>The full presentation video is coming soon.</b></i> -->
   <div class="wrapper">
     <!-- <div class="col-xs-4 col-lg-6;"> -->
      <p style="text-align: justify">
        <br> We propose <b>MAtCha Gaussians</b>, a novel surface representation for <b>reconstructing 
        high-quality 3D meshes with photorealistic rendering from sparse-view images</b>.
        Our key idea is to model the underlying scene geometry as an <b>Atlas of Charts</b> which we render with 2D 
        <b>Gaussian surfels</b>.
        We initialize the charts with a <b>monocular depth estimation model</b> and refine them using 
        <b>differentiable Gaussian rendering</b> and a <b>lightweight neural chart deformation model</b>.
        Combined with a sparse-view SfM model like 
        <a href="https://europe.naverlabs.com/research/publications/mast3r-sfm-a-fully-integrated-solution-for-unconstrained-structure-from-motion/">MASt3R-SfM</a>, 
        MAtCha can recover <b>sharp and accurate surface meshes</b> 
        of both <b>foreground and background objects</b> in <b>unbounded scenes</b> within minutes, only from 
        <b>a few unposed RGB images</b>. 
      </p>
     <!-- </div> -->
   </div>
   </div>
 </div>
</div>

<div class="container">
  <h2>Updates</h2>
  <hr/>
  <p>
    <ul>
      <li> <b>12-2024</b>: Initial release of the paper. </li>
    </ul>
  </p>
<!-- </div> -->
<p><br></p>

<!-- <div class="container"> -->
 <h2>Overview</h2>
 <hr/>
 <div id="resultsCarousel" class="carousel slide" data-ride="carousel" data-interval="30000">
  <!-- Indicators -->
  <ol class="carousel-indicators">
    <li data-target="#resultsCarousel" data-slide-to="0" class="active"></li>
    <li data-target="#resultsCarousel" data-slide-to="1"></li>
    <li data-target="#resultsCarousel" data-slide-to="2"></li>
    <li data-target="#resultsCarousel" data-slide-to="3"></li>
    <li data-target="#resultsCarousel" data-slide-to="4"></li>
    <li data-target="#resultsCarousel" data-slide-to="5"></li>
    <!-- Add more indicators as needed -->
  </ol>

  <!-- Wrapper for slides -->
  <div class="carousel-inner">
    
    <!-- Garden -->
    <div class="item active">
      <iframe src="https://drive.google.com/file/d/1HrFkHNZqX5umnjAMb3FuOhLlCH2Xl4Ou/preview" 
            style="width:100%; height:310px" 
            frameborder="0"
            allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"
            allowfullscreen></iframe>
      <div class="carousel-caption">
        <h3>Garden (Mip-NeRF 360) - 10 training images</h3>
      </div>
    </div>

    <!-- Bicycle -->
    <div class="item">
      <iframe src="https://drive.google.com/file/d/11UgQilGKh7W_uCeYW0SrdksY7WQSyAsG/preview" 
            style="width:100%; height:310px" 
            frameborder="0"
            allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"
            allowfullscreen></iframe>
      <div class="carousel-caption">
        <h3>Bicycle (Mip-NeRF 360) - 10 training images</h3>
      </div>
    </div>

    <!-- Bonsai -->
    <div class="item">
      <iframe src="https://drive.google.com/file/d/1UQDs7NLkSk2ddybW1_b7Rbtw8Vl7Epeu/preview" 
            style="width:100%; height:310px" 
            frameborder="0"
            allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"
            allowfullscreen></iframe>
      <div class="carousel-caption">
        <h3>Bonsai (Mip-NeRF 360) - 5 training images</h3>
      </div>
    </div>

    <!-- Scan 21 -->
    <div class="item">
         <iframe src="https://drive.google.com/file/d/1DAsBDYGShrVTVpmTgiTr0QqLvWE5emha/preview" 
            style="width:100%; height:310px" 
            frameborder="0"
            allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"
            allowfullscreen></iframe>
      <div class="carousel-caption">
        <h3>Scan 21 (DTU) - 3 training images</h3>
      </div>
    </div>

    <!-- Gundam -->
    <div class="item">
          <iframe src="https://drive.google.com/file/d/1eK59AsKKCX1FEYH8FL-xdnJblFF_yvH0/preview" 
            style="width:100%; height:310px" 
            frameborder="0"
            allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"
            allowfullscreen></iframe>
        <div class="carousel-caption">
          <h3>Gundam (Custom Scene) - 10 training images</h3>
        </div>
    </div>

    <!-- Scan 24 -->
    <div class="item">
      <iframe src="https://drive.google.com/file/d/1Q4SsqI1dg8zu02xoGmx7XQnpRVdbC8Ua/preview" 
         style="width:100%; height:310px" 
         frameborder="0"
         allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"
         allowfullscreen></iframe>
   <div class="carousel-caption">
     <h3>Scan 24 (DTU) - 3 training images</h3>
   </div>
 </div>
    
  </div>

  <!-- Left and right controls -->
  <a class="left carousel-control" href="#resultsCarousel" data-slide="prev">
    <span class="glyphicon glyphicon-chevron-left"></span>
    <span class="sr-only">Previous</span>
  </a>
  <a class="right carousel-control" href="#resultsCarousel" data-slide="next">
    <span class="glyphicon glyphicon-chevron-right"></span>
    <span class="sr-only">Next</span>
  </a>
</div>
<br>

<p>
    We present a novel appearance model that simultaneously realizes explicit <b>high-quality 3D surface mesh recovery</b> 
    and <b>photorealistic novel view synthesis</b> from <b>sparse view samples</b>. <br><br>

    Our key idea is to model the underlying scene geometry <b>M</b>esh as an <b>At</b>las of <b>Cha</b>rts 
    which we render with 2D <b>Gaussian</b> surfels (MAtCha Gaussians).<br><br>

    MAtCha distills <b>high-frequency scene surface details</b> from an off-the-shelf <b>monocular depth estimator</b> 
    and refines it through <a href="https://surfsplatting.github.io/">2D Gaussian surfel rendering</a>.     
    The Gaussian surfels are attached to the charts on the fly, satisfying photorealism of neural volumetric rendering 
    and crisp geometry of a mesh model, <i>i.e.</i>, two seemingly contradicting goals in a single model. <br><br>
    
    At the core of MAtCha lies a <b>novel neural deformation model</b> and a <b>structure loss</b> that preserve the fine surface details 
    distilled from learned monocular depths while addressing their fundamental scale ambiguities. <br><br>

    Results of extensive experimental validation demonstrate MAtCha's state-of-the-art quality of surface reconstruction 
    and photorealism on-par with top contenders but with dramatic reduction in the number of input views and computational time.<br>
    
    We believe MAtCha will serve as a foundational tool for any visual application in vision, graphics, 
    and robotics that require explicit geometry in addition to photorealism. 
 </p>
<!-- </div> -->

 <p><br></p>

 <h2>Optimizing Charts in a Sparse-View Scenario</h2>
 <hr/>

 <p>
  Given a few RGB images and their camera poses obtained using a sparse-view SfM method such as 
  <a href="https://europe.naverlabs.com/research/publications/mast3r-sfm-a-fully-integrated-solution-for-unconstrained-structure-from-motion/">MASt3R-SfM</a>, 
  we first initialize charts using a 
  <a href="https://depth-anything-v2.github.io/">pretrained monocular depth estimation model</a>. 
  Each chart is represented as a mesh equipped with a UV map, mapping a 2D plane to the 3D surface.
 </p>
 <div class="col-xs-12 col-lg-12" style="text-align:center">
  <!-- <img src="https://anttwo.github.io/matcha/images/pipeline/pipeline.png" -->
  <img src="./images/pipeline/pipeline.png" 
  alt="pipeline.png" class="text-center" style="width: 85%; max-width: 1920px">
  <br><br>
 </div>
 <p>
    We then optimize our charts and enforce their alignment with input SfM data using three key components: 
    <!-- Bullet points with enumerate -->
    <ul>
      <li> <b>Depth encodings</b> stored along a 1D axis for encouraging points with similar initial depth to be deformed together.</li>
      <li> <b>Charts encodings</b> stored in a sparse 2D grid in UV space for efficiently deforming the geometry 
        while preserving high-frequency surface details visible in the initial depth maps.</li>
      <li> <b>Confidence maps</b> for each chart, for automatically identifying outliers in the input SfM data.</li>
    </ul>
    Both depth and charts encodings are fed into a <b>tiny MLP</b> that outputs a <b>deformation field</b> for each chart. 
    We encourage the deformation field to not only <b>align the charts with the input SfM data</b>, but also to 
    create a <b>coherent manifold structure</b> where charts are aligned with each other.<br><br>

    Our aligned charts provide a sharp, dense and accurate estimate of the 3D scene, 
    which can be further <b>refined</b> using input images and a <b><a href="https://surfsplatting.github.io/">Gaussian Splatting</a>-based rendering pipeline</b>. 
    
    Our representation allows for reconstructing high-quality surface meshes within minutes, even in sparse-view scenarios.
 </p>
 
 <h2>Extracting Meshes from MAtCha Gaussians</h2>
 <hr/>
 <div class="row" style="text-align:center">
  <div class="col-xs-6 col-lg-6">
    <!-- <img src="https://anttwo.github.io/matcha/images/mesh_extraction/tsdf_multires.png"  -->
    <img src="./images/mesh_extraction/tsdf_multires.png"
    alt="tsdf_multires.png" class="text-center" style="width: 100%; max-width: 1000px">
    <br><br>
    <img src="./images/mesh_extraction/tsdf_multires2.png"
    alt="tsdf_multires2.png" class="text-center" style="width: 100%; max-width: 1000px">
   <p style="text-align:center">(a) Multi-resolution TSDF fusion (10 training views)</p>
 </div>
 <div class="col-xs-6 col-lg-6">
  <!-- <img src="https://anttwo.github.io/matcha/images/mesh_extraction/adaptive_tetra.png"  -->
  <img src="./images/mesh_extraction/adaptive_tetra.png"
  alt="adaptive_tetra.png" class="text-center" style="width: 100%; max-width: 1000px">
  <br><br>
  <img src="./images/mesh_extraction/adaptive_tetra2.png"
  alt="adaptive_tetra2.png" class="text-center" style="width: 100%; max-width: 1000px">
 <p style="text-align:center">(b) Adaptive tetrahedralization (10 training views)</p>
</div>
</div>
<br>

 <p>
  Most existing methods relying on 3D Gaussians or <a href="https://surfsplatting.github.io/">2D Gaussian Surfels</a> 
  apply TSDF fusion on rendered depth maps to extract a mesh from the volumetric representation.
  However, TSDF fusion is limited to bounded scenes and does not allow for extracting high-quality meshes 
  including both foreground and background objects of the scene. 
  Moreover, applying TSDF fusion on 2D Gaussian Surfels can over-smooth the geometry, erode fine details, 
  and produce artifacts, such as <i>"disk-aliasing"</i> patterns on the surface.<br><br>

  In this regard, while we propose <b>(a) a custom multi-resolution TSDF fusion</b> including foreground and background objects in our implementation, 
  we also propose <b>(b) to adapt the tetrahedralization from 
  <a href="https://niujinshuchong.github.io/gaussian-opacity-fields/">Gaussian Opacity Fields (GOF)</a></b>
  to make it compatible with any Gaussian-based method capable of rendering perspective-accurate depth maps.

  <ul>
    <li>
      First, we propose to change the definition of the opacity field, 
      using depth maps instead of 3D Gaussians as in GOF:
      For any set of input depth maps, we define a binary opacity field from the depth maps as well as an adaptive dilation operation 
      to avoid eroding geometry during mesh extraction.
    </li>
    <li>
      Second, because the tetrahedralization introduced in GOF generally produces very large meshes with more than 10M vertices, 
      we propose a new sampling strategy to build the initial tetrahedron grid to easily adjust or lower the resolution of the output mesh.
    </li>
  </ul>

  We believe that our adaptation of GOF tetrahedralization provides a high-quality alternative to TSDF fusion and
  can generalize to most Gaussian-based surface reconstruction methods.
 </p>

 <h2>BibTex</h2>
 <hr/>
 If you find this work useful for your research, please cite:
 <br><br>

 <div class="card">
   <div class="card-block">
     <pre class="card-text clickselect">
      @article{guedon2024matcha,
        title={MAtCha Gaussians: Atlas of Charts for High-Quality Geometry and Photorealism From Sparse Views}, 
        author={Gu{\'e}don, Antoine and Ichikawa, Tomoki and Yamashita, Kohei and Nishino, Ko},
        journal={arXiv},
        year={2024},
      }
    </pre>
   </div>
 </div>

 <p><br></p>

 <h2>Further information</h2>
 <hr/>
 If you like this project, check out our previous works related to 3D reconstruction and Gaussian Splatting:
 <ul>
    <li>
      <a href="https://anttwo.github.io/frosting/">Gu&eacutedon and Lepetit. - 
        Gaussian Frosting: Editable Complex Radiance Fields with Real-Time Rendering
        (ECCV 2024 - Oral)</a>
    </li>
   <li>
    <a href="https://anttwo.github.io/sugar/">Gu&eacutedon and Lepetit. - 
      SuGaR: Surface-Aligned Gaussian Splatting for Efficient 3D Mesh Reconstruction and High-Quality Mesh Rendering
      (CVPR 2024)</a>
   </li>
 </ul>

 <p><br></p>

 <h2>Acknowledgements</h2>
 <hr/>
 <p>
  This work was in part supported by 
  <b>JSPS 20H05951</b> and <b>21H04893</b>, 
  and <b>JST JPMJCR20G7</b> and <b>JPMJAP2305</b>. <br>
  
  This work was also in part supported by the <b>ERC grant "explorer" (No. 101097259)</b>.<br>
  
  This work was granted access to the HPC resources of <b>IDRIS</b> under the allocation 
  <b>2024-AD011013387R2</b> made by <b>GENCI</b>.
 </p>
</div>

<div class="container" style="padding-top:3rem; padding-bottom:3rem">
 <p style="text-align:center">
   &#169; You are welcome to copy the code of the webpage, please attribute the source with a link
   back to this page and remove the analytics.<br>
 </p>
</div>

<!-- Dark mode toggle -->
<script>
  const themeToggle = document.createElement('button');
  themeToggle.className = 'theme-toggle';
  themeToggle.innerHTML = '<span class="material-icons-outlined">dark_mode</span>';

  themeToggle.addEventListener('click', () => {
      const currentTheme = document.documentElement.getAttribute('data-theme');
      const newTheme = currentTheme === 'dark' ? 'light' : 'dark';
      document.documentElement.setAttribute('data-theme', newTheme);
      localStorage.setItem('theme', newTheme);
      themeToggle.innerHTML = newTheme === 'dark' ? '<span class="material-icons-outlined">light_mode</span>' : '<span class="material-icons-outlined">dark_mode</span>';
  });

  // Check for saved theme preference
  const savedTheme = localStorage.getItem('theme') || 'dark';
  document.documentElement.setAttribute('data-theme', savedTheme);
  themeToggle.innerHTML = savedTheme === 'dark' ? '<span class="material-icons-outlined">light_mode</span>' : '<span class="material-icons-outlined">dark_mode</span>';

  document.body.appendChild(themeToggle);
</script>

</body>
</html>